{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS231n Convulutional Neural Networks for Visual Recognition\n",
    "http://cs231n.github.io/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Multiplication\n",
    "\n",
    "1. Make sure that the the number of columns in the 1st one equals the number of rows in the 2nd one\n",
    "2. Multiply the elements of each row of the first matrix by the elements of each column in the second matrix.\n",
    "3. Add the products.\n",
    "4. Output = #rows x #cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[37, 49],\n",
       "        [70, 94]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.matrix( ((2,3,4), (5,6,7)) ) 2x3\n",
    "y = np.matrix( ((2, 3), (3,5), (6,7)) ) 3x2\n",
    "\n",
    "x * y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear classifier\n",
    "http://cs231n.github.io/linear-classify/\n",
    "\n",
    "One  interpretation for the weights WW is that each row of WW corresponds to a template (or sometimes also called a prototype) for one of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ -96.8 ],\n",
       "        [ 437.9 ],\n",
       "        [  60.75]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.matrix( [[0.2, -0.5, 0.1, 2.0], #cat\n",
    "                [1.5, 1.3, 2.1, 0.0],  #dog\n",
    "               [0, 0.25, 0.2, -0.3]] )  #ship\n",
    "x = np.matrix( [[56], \n",
    "                [231], \n",
    "                [24], \n",
    "                [2]] ) \n",
    "\n",
    "b = np.matrix ([[1.1], \n",
    "               [3.2],\n",
    "               [-1.2]])\n",
    "\n",
    "W*x + b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is a little cumbersome to keep track of two sets of parameters (the biases bb and weights WW) separately. A commonly used trick is to combine the two sets of parameters into a single matrix that holds both of them by extending the vector xixi with one additional dimension that always holds the constant 1 - a default bias dimension. With the extra dimension, the new score function will simplify to a single matrix multiply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ -96.8 ],\n",
       "        [ 437.9 ],\n",
       "        [  60.75]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.matrix( [[0.2, -0.5, 0.1, 2.0, 1.1], #cat + bias\n",
    "                [1.5, 1.3, 2.1, 0.0, 3.2],  #dog + bias\n",
    "               [0, 0.25, 0.2, -0.3, -1.2]] ) #ship + bias\n",
    "                 \n",
    "x = np.matrix( [[56], \n",
    "                [231], \n",
    "                [24], \n",
    "                [2],\n",
    "               [1]] ) #default bias \n",
    "\n",
    "W*x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classification Demo\n",
    "http://vision.stanford.edu/teaching/cs231n/linear-classify-demo/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss\n",
    "https://github.com/pmaroun/matlab-ML-excercises/blob/master/ex3/lrCostFunction.m\n",
    "    \n",
    "In Defense of One vs All: http://www.jmlr.org/papers/volume5/rifkin04a/rifkin04a.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Multi-class Support Vector Machine Loss\n",
    "The SVM loss is set up so that the SVM “wants” the correct class for each image to a have a score higher than the incorrect classes by some fixed margin Δ.\n",
    "http://cs231n.github.io/linear-classify/#loss\n",
    "\n",
    "max(0, -) commonly refered to as <b>hinge loss</b>\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def L_i(x, y, W):\n",
    "  \"\"\"\n",
    "  unvectorized version. Compute the multiclass svm loss for a single example (x,y)\n",
    "  - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)\n",
    "    with an appended bias dimension in the 3073-rd position (i.e. bias trick)\n",
    "  - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)\n",
    "  - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)\n",
    "  \"\"\"\n",
    "  delta = 1.0 # see notes about delta later in this section\n",
    "  scores = W.dot(x) # scores becomes of size 10 x 1, the scores for each class\n",
    "  \n",
    "  correct_class_score = scores[y]\n",
    "  D = W.shape[0] # number of classes, e.g. 10\n",
    "  loss_i = 0.0\n",
    "  for j in xrange(D): # iterate over all wrong classes\n",
    "    if j == y:\n",
    "      # skip for the true class to only loop over incorrect classes\n",
    "      continue\n",
    "    # accumulate loss for the i-th example\n",
    "    loss_i += max(0, scores[j] - correct_class_score + delta)\n",
    "  return loss_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3, 1)\n",
      "[[ 694.25]]\n",
      "shape: (3, 1)\n",
      "0.0\n",
      "shape: (3, 1)\n",
      "[[ 378.15]]\n"
     ]
    }
   ],
   "source": [
    "W = np.matrix( [[0.2, -0.5, 0.1, 2.0, 1.1], #cat + bias\n",
    "                [1.5, 1.3, 2.1, 0.0, 3.2],  #dog + bias\n",
    "               [0, 0.25, 0.2, -0.3, -1.2]] ) #ship + bias\n",
    "                 \n",
    "x = np.matrix( [[56], \n",
    "                [231], \n",
    "                [24], \n",
    "                [2],\n",
    "               [1]] ) #default bias \n",
    "y = 0\n",
    "print L_i(x, y, W)\n",
    "\n",
    "y = 1\n",
    "print L_i(x, y, W)\n",
    "\n",
    "y = 2\n",
    "print L_i(x, y, W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_i_vectorized(x, y, W):\n",
    "  \"\"\"\n",
    "  A faster half-vectorized implementation. half-vectorized\n",
    "  refers to the fact that for a single example the implementation contains\n",
    "  no for loops, but there is still one loop over the examples (outside this function)\n",
    "  \"\"\"\n",
    "  delta = 1.0\n",
    "  scores = W.dot(x)\n",
    "  # compute the margins for all classes in one vector operation\n",
    "  margins = np.maximum(0, scores - scores[y] + delta)\n",
    "  # on y-th position scores[y] - scores[y] canceled and gave delta. We want\n",
    "  # to ignore the y-th position and only consider margin on max wrong class\n",
    "  margins[y] = 0\n",
    "  loss_i = np.sum(margins)\n",
    "  return loss_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "694.25\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "W1 = np.matrix( [[0.2, -0.5, 0.1, 2.0, 1.1], #cat + bias\n",
    "                [1.5, 1.3, 2.1, 0.0, 3.2],  #dog + bias\n",
    "               [0, 0.25, 0.2, -0.3, -1.2]] ) #ship + bias\n",
    "\n",
    "W2 = np.matrix( [[0.1, -0.4, 0.6, 1.0, 2.1], #cat + bias\n",
    "                [1.1, 1.7, 1.1, 0.1, 2.2],  #dog + bias\n",
    "               [1, 0.55, 0.4, -0.5, -1.7]] ) #ship + bias\n",
    "\n",
    "W = np.row_stack([W1, W2])\n",
    "\n",
    "x1 = np.matrix( [[56], \n",
    "                [231], \n",
    "                [24], \n",
    "                [2],\n",
    "               [1]] ) #default bias \n",
    "x2 = np.matrix( [[231], \n",
    "                [76], \n",
    "                [12], \n",
    "                [1],\n",
    "               [1]] ) #default bias \n",
    "X = np.column_stack([x1, x2])\n",
    "\n",
    "y = [0, 1]\n",
    "\n",
    "for i in range(0, 2):\n",
    "    print L_i_vectorized(\n",
    "        X[:, i:i+1],\n",
    "        y[i:i+1],\n",
    "        W1 ##W[i*(i+2):(i+3)*(i+1),:]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    fully-vectorized implementation :\n",
    "    - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10)\n",
    "    - y is array of integers specifying correct class (e.g. 50,000-D array)\n",
    "    - W are weights (e.g. 10 x 3073)\n",
    "1. Calculate the W.x matrix multiplication result.\n",
    "2. Create a column vector of the correct scores in each row, using the values in y to index the columns.\n",
    "3. Subtract the correct scores from each of the scores in the W.x matrix. CHECK: The correct class elements are now 0.\n",
    "4. Apply the max(0, scores difference +1) to each element.\n",
    "5. The correct class elements will now be 1, set them back to 0.\n",
    "6. Sum up all elements in the matrix, divide by the number of examples to normalize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def L(X, y, W):\n",
    "    # evaluate loss over all examples in X without using any for loops\n",
    "    # left as exercise to reader in the assignment\n",
    "    delta=1.0\n",
    "    scores = W.dot(X) # scores : 10x50000\n",
    "    N=X.shape[1] # 50000\n",
    "    z=np.arange(N)\n",
    "    correct_class_score=scores[y,z].reshape(1,N)  \n",
    "    loss=np.sum(np.maximum(0,(scores[:,]-correct_class_score[0,]+delta)),axis=0)\n",
    "    loss-=delta\n",
    "    return loss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 694.25    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "W1 = np.array( [[0.2, -0.5, 0.1, 2.0, 1.1], #cat + bias\n",
    "                [1.5, 1.3, 2.1, 0.0, 3.2],  #dog + bias\n",
    "               [0, 0.25, 0.2, -0.3, -1.2]] ) #ship + bias\n",
    "\n",
    "W2 = np.array( [[0.1, -0.4, 0.6, 1.0, 2.1], #cat + bias\n",
    "                [1.1, 1.7, 1.1, 0.1, 2.2],  #dog + bias\n",
    "               [1, 0.55, 0.4, -0.5, -1.7]] ) #ship + bias\n",
    "\n",
    "W = np.array([W1, W2])\n",
    "\n",
    "x1 = np.matrix( [[56], \n",
    "                [231], \n",
    "                [24], \n",
    "                [2],\n",
    "               [1]] ) #default bias \n",
    "x2 = np.matrix( [[231], \n",
    "                [76], \n",
    "                [12], \n",
    "                [1],\n",
    "               [1]] ) #default bias \n",
    "X = np.column_stack([x1, x2])\n",
    "\n",
    "y = [0, 1]\n",
    "\n",
    "print L(X, y, W1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Softmax Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike SVM, which produces difficult to interpret scores, Softmax produces normlaized class probabilities.  These probabilities are better thought of as confidences.  We are replacing <b>hinge loss</b> with <b>cross entropy loss</b>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
